{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png\" align=left>\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png\" align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to setup data path\n",
    "import os\n",
    "\n",
    "datapath = os.getcwd()\n",
    "if datapath.find('databricks') != -1:\n",
    "    ACCESS_KEY = \"AKQQ\"\n",
    "    SECRET_KEY = \"YJbbm\"\n",
    "    AWS_BUCKET_NAME = \"nyc\"\n",
    "    datapath = \"s3a://%s:%s@%s/\" %(ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Lab: Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will build on the techniques covered in the Spark intro to develop a simple word count application. In this lab, we will write code that calculates the most common words in a text.\n",
    " \n",
    "the goal is to learn the distribution of letters in the most popular words in a corpus\n",
    "This could also be scaled to find the most common words in Wikipedia.\n",
    "\n",
    "** Here're the suggested steps: **\n",
    "\n",
    "* *Step 1:* Creating a base RDD from a python list\n",
    "* *Step 2:* Text cleaning\n",
    "* *Step 3:* Counting unique words\n",
    "* *Step 4:* Putting them together\n",
    "* *Step 5:* Writing a self-contained applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1: Creating a pair RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** We'll start by generating a base RDD from the file `pyspark_1/README.md` by using the `sc.textFile()` method. Then we'll print out the type of the base RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# TODO: create a spark RDD `lineRDD` with lines.\n",
    "\n",
    "filePath = os.path.join(datapath, \"pyspark_1/README.md\")\n",
    "\n",
    "lineRDD = sc.textFile(filePath)\n",
    "print type(lineRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**1.2** Remember all transformations in Spark are lazy, in that they do not compute their results right away until an action requires a result to be returned to the driver program. To make errors easier to localize, we can apply an action to `lineRDD` to see if it works properly.\n",
    "\n",
    "Usually, `collect()` method is **NOT** recommended as it will collect all the data to the driver node. Now apply `take(5)` to the `lineRDD` you just created and see if the file has been load successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'# Apache Spark',\n",
       " u'',\n",
       " u'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " u'supports general computation graphs for data analysis. It also supports a']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: test lineRDD by applying an action `take(5)` to it.\n",
    "\n",
    "# Your code goes here\n",
    "lineRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Before moving to further analysis, we want to clean the text a little bit. Here're some issues that you may want to address:\n",
    "\n",
    "* All punctuation should be removed.\n",
    "* All words should be in lower case so the same words of different cases are not considered to be different.\n",
    "* All leading or trailing spaces should be removed.\n",
    "\n",
    "Define the function cleanText to do the work. You may want to use the Python `re` and/or `string` module to remove punctuation. Then test `cleanTest()` with `linesRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev run tests\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "\n",
    "def cleanText(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation, change to lower case, and strips white spaces\n",
    "    \n",
    "    :param text(str): A string.\n",
    "    :return: the cleaned string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code goes here\n",
    "    cleaned_text = re.sub(\"\\W\", \" \", text).lower().strip()\n",
    "    return cleaned_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test cleanText() with texts in lines\n",
    "    lines = lineRDD.takeSample(False, 3)\n",
    "    print cleanText(lines[0])\n",
    "    print cleanText(lines[1])\n",
    "    print cleanText(lines[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Now create another RDD with the name `lineCleaned` by applying the `cleanText()` to `lineRDD`. To test the result, use `take(5)` to print the first 5 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'apache spark', u'', u'spark is a fast and general cluster computing system for big data  it provides', u'high level apis in scala  java  python  and r  and an optimized engine that', u'supports general computation graphs for data analysis  it also supports a']\n"
     ]
    }
   ],
   "source": [
    "# TODO: create lineCleaned and then test it with take(5)\n",
    "\n",
    "lineCleaned = lineRDD.map(cleanText) #<fill in>\n",
    "print lineCleaned.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Counting unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** To count the number of occurence of each word, we can:\n",
    "\n",
    "* Create an RDD `wordRDD` by splitting the lines to words using `flatMap()`.\n",
    "* Create a pair RDD `wordPair` with value of each word equals 1.\n",
    "* Create another pair RDD `wordCount` by aggregrating the values of the same word in `wordPair`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'apache', 1), (u'spark', 1), (u'spark', 1), (u'is', 1), (u'a', 1)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: first create `wordRDD` and then `wordPair`.\n",
    "\n",
    "wordRDD = lineCleaned.flatMap(lambda line: line.split()) #<fill in>\n",
    "wordPair = wordRDD.map(lambda word: (word, 1)) #<fill in>\n",
    "\n",
    "print wordPair.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'please', 3), (u'through', 1), (u'computation', 1), (u'1', 1), (u'mesos', 1)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: create `wordCount` by adding values of the same words in `wordPair`.\n",
    "\n",
    "# Hint: you can use either `groupByKey()` followed by `mapValues()` or `reduceByKey()`\n",
    "\n",
    "from operator import add\n",
    "wordCount = wordPair.reduceByKey(add) #<fill in>\n",
    "\n",
    "print wordCount.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Usually the frequency reflects how important a word is to a document. We can view the most frequent words by using the [`takeOrdered()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeOrdered) action; however, since the elements of the RDD are pairs, we need a custom sort function that sorts using the value part of the pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'spark', 33), (u'the', 24), (u'to', 16), (u'for', 14), (u'run', 13), (u'apache', 13), (u'and', 12), (u'org', 11), (u'you', 9), (u'a', 9)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: use `takeOrdered()` to get the top 10 most frequent word and their counts\n",
    "\n",
    "frequentWord = wordCount.takeOrdered(10, key=lambda kv: -kv[1]) #<fill in>\n",
    "print frequentWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** (Optional) You may notice that some of the most frequent words are stopwords - words which do not contain important significance. Sometimes you may want to remove the stopwords since they usually do not carry much useful information. \n",
    "\n",
    "If you decide to do so, use the `stopwords.txt` file in the directory `pyspark_1`. Then remove the stopwords by applying a filter to the RDD you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'spark', 33), (u'run', 13), (u'apache', 13), (u'org', 11), (u'building', 8), (u'example', 8), (u'hadoop', 7), (u'maven', 6), (u'http', 6), (u'using', 6)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: recreate `wordCount` by removing the stopwords. The filter can be applied to different places.\n",
    "\n",
    "STOPWORDS_PATH = os.path.join(datapath, \"pyspark_1/stopwords.txt\")\n",
    "\n",
    "cachedStopWords = sc.broadcast(sc.textFile(STOPWORDS_PATH).collect())\n",
    "\n",
    "wordCount_removed = wordCount.filter(lambda kv: kv[0] not in cachedStopWords.value) #<fill in>\n",
    "print wordCount_removed.takeOrdered(10, key=lambda kv: -kv[1]) #<fill in>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Putting them together\n",
    "\n",
    "Now we can define a function `wordCount()` to count the frequencies of unique words in an RDD of text by putting all the transformations from step 1 throuth step 3 together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: complete the wordCount() function\n",
    "\n",
    "def wordCount(lineRDD):\n",
    "    \"\"\"\n",
    "    Creates a pair RDD with word counts from an RDD of text.\n",
    "    :param wordListRDD(pyspark.rdd.RDD): An RDD of text.\n",
    "    :return: Return the count of each unique word in this RDD as a pair RDD\n",
    "    \"\"\"\n",
    "    wordCounts = (lineRDD\n",
    "                  .map(cleanText)\n",
    "                  .flatMap(lambda line: line.split())\n",
    "                  .filter(lambda word: word not in cachedStopWords.value)\n",
    "                  .map(lambda word: (word, 1))\n",
    "                  .reduceByKey(add))\n",
    "    return wordCounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark', 33),\n",
       " (u'run', 13),\n",
       " (u'apache', 13),\n",
       " (u'org', 11),\n",
       " (u'building', 8),\n",
       " (u'example', 8),\n",
       " (u'hadoop', 7),\n",
       " (u'maven', 6),\n",
       " (u'http', 6),\n",
       " (u'using', 6)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: test the function `wordCount()` with `takeOrdered()`\n",
    "\n",
    "# Your code goes here\n",
    "wordCount(lineRDD).takeOrdered(10, key=lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Writing a self-contained applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write a self-contained application using the Spark API. The app should read a text file and return the word counts.\n",
    "\n",
    "Note that we need to create a `SparkContext` object, which tells Spark how to access a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"pysparkWordCount\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your `wordcount.py` file from scratch. When you finish, use `spark-submit wordcount.py <text file>` to run it from command line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
