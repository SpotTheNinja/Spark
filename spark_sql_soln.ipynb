{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png\" align=left>\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png\" align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spark Tutorial: Spark SQL and DataFrames**\n",
    "\n",
    "## 0. Overview\n",
    "\n",
    "Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. \n",
    "\n",
    "There are several ways to interact with Spark SQL including SQL and the Dataset API. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.\n",
    "\n",
    "### 0.1 SQL\n",
    "\n",
    "One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. When running SQL from within another programming language the results will be returned as a DataFrame.\n",
    "\n",
    "### 0.2 DataFrames\n",
    "\n",
    "A DataFrame is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. \n",
    "\n",
    "DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to setup data path\n",
    "import os\n",
    "\n",
    "datapath = os.getcwd()\n",
    "if datapath.find('databricks') != -1:\n",
    "    ACCESS_KEY = \"AKIAI2P5MSEO2JYXJVQQ\"\n",
    "    SECRET_KEY = \"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\"\n",
    "    AWS_BUCKET_NAME = \"nycdsabootcamp\"\n",
    "    datapath = \"s3a://%s:%s@%s/\" %(ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Starting Point: SparkSession\n",
    "\n",
    "`SparkSession` is the entry point to programming Spark with the DataFrame API.\n",
    "\n",
    "SparkSession in Spark 2.0 provides builtin support for Hive features including the ability to write queries using HiveQL, access to Hive UDFs, and the ability to read data from Hive tables. To use these features, you do not need to have an existing Hive setup.\n",
    "\n",
    "In pyspark, `spark` is an existing `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If launching applications with spark-submit, then we need to create a basic SparkSession using `SparkSession.builder()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't Run!\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Working with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrames` can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n",
    "\n",
    "Here we will be using the file `./ml-100k/u.user` as an example to illustrate how we can create a DataFrame from an existing RDD. \n",
    "\n",
    "The column names are: `userId | age | gender | occupation | zipCode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(datapath, \"./ml-100k/u.user\")\n",
    "\n",
    "userRDD = sc.textFile(filepath)\n",
    "print userRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parts = userRDD.map(lambda l: l.split(\"|\"))\n",
    "print parts.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Interoperating with RDDs\n",
    "\n",
    "Spark SQL supports two different methods for converting existing RDDs into DataFrames. \n",
    "\n",
    "* uses reflection to infer the schema of an RDD that contains specific types of objects. \n",
    "\n",
    "* through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD.\n",
    "\n",
    "**Inferring the Schema Using Reflection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark is an existing SparkSession.\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert each line to a Row.\n",
    "user = parts.map(lambda p: Row(userId=int(p[0]), \n",
    "                               age=int(p[1]),\n",
    "                               gender=p[2],\n",
    "                               occupation=p[3],\n",
    "                               zipCode=p[4]))\n",
    "\n",
    "# Infer the schema.\n",
    "userDF = spark.createDataFrame(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Once a DataFrame has been created, we can check it's schema.\n",
    "userDF.printSchema()\n",
    "\n",
    "# Instead of using take(), a better way is to use show() to display the first few rows.\n",
    "userDF.show(5)\n",
    "\n",
    "# DataFrame.dgtypes returns a list of tuples that contains the datatype of each column object.\n",
    "print userDF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programmatically Specifying the Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "userSchema = StructType([StructField(\"userId\", IntegerType(), True),\n",
    "                         StructField(\"age\", IntegerType(), True),\n",
    "                         StructField(\"gender\", StringType(), True),\n",
    "                         StructField(\"occupation\", StringType(), True),\n",
    "                         StructField(\"zipCode\", StringType(), True)]) # Note: some zipCodes are strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typedPart = parts.map(lambda p: (int(p[0]), int(p[1]), p[2], p[3], p[4]))\n",
    "userDFb = spark.createDataFrame(typedPart, userSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userDFb.printSchema()\n",
    "userDFb.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "The u.data contains 100000 ratings by 943 users on 1682 items:\n",
    "\n",
    "* path=\"./ml-100k/u.data\"\n",
    "* column name: userId | itemId | rating | timestamp\n",
    "* sample row: u'196\\t242\\t3\\t881250949'\n",
    "  \n",
    "1.1 Create a DataFrame called `ratingDF` with the `u.data` file from the `path` above.\n",
    "\n",
    "1.2 Show schema and the first 5 rows of `ratingDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "ratingPath = os.path.join(datapath, \"./ml-100k/u.data\")\n",
    "\n",
    "# 1.1\n",
    "ratingSchema = StructType([StructField(\"userId\", IntegerType(), True),\n",
    "                           StructField(\"movieId\", IntegerType(), True),\n",
    "                           StructField(\"rating\", IntegerType(), True),\n",
    "                           StructField(\"timestamp\", IntegerType(), True)])\n",
    "\n",
    "ratingDF = spark.read.csv(path=ratingPath, sep=u\"\\t\", schema=ratingSchema)\n",
    "\n",
    "# 1.2 \n",
    "ratingDF.printSchema()\n",
    "ratingDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 DataFrames Operations\n",
    "\n",
    "Here we will perform some basic DataFrames operations in pyspark. For a complete list of the types of operations that can be performed on a DataFrame refer to the [API Documentation](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`columns` returns all column names as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`describe(*cols)`\n",
    "shows statistics for numeric columns. This include count, mean, stddev, min, and max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userDF.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selection**\n",
    "\n",
    "select(*cols)\n",
    "projects a set of expressions and returns a new DataFrame.\n",
    "\n",
    "drop(col)\n",
    "returns a new DataFrame that drops the specified column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cols can be column names or column objects\n",
    "# in the following example \"userId\" is column name\n",
    "# userDF.gender and userDF[\"occupation\"] are both column objects\n",
    "userDF.select(\"userId\", userDF.gender, userDF[\"age\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userDF.drop(\"zipCode\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# When using column object, we can also apply operations on it very easily.\n",
    "\n",
    "# cast(dataType) convert the column into type dataType.\n",
    "# between(lowerBound, upperBound) returns true if the value of this \n",
    "# expression is between the given columns.\n",
    "\n",
    "userDF.select(userDF[\"userId\"].cast(\"string\"), userDF[\"age\"].between(20, 40)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there's no difference to access a DataFrame’s columns either by attribute (df.age) or by indexing (df['age']), users are highly encouraged to use the latter form, which is future proof and won’t break with column names that are also attributes on the DataFrame class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter**\n",
    "\n",
    "filter(condition)\n",
    "filters rows using the given condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using a Column of types.BooleanType as condition\n",
    "userDF.filter(userDF[\"occupation\"] == \"programmer\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using a string of SQL expression\n",
    "userDF.filter(\"age between 20 and 40\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grouping and Aggregation**\n",
    "\n",
    "`groupBy(*cols)`\n",
    "groups the DataFrame using the specified columns, so we can run aggregation on them.\n",
    "\n",
    "The available aggregation functions include:\n",
    "* `avg(*cols)`\n",
    "* `count()`\n",
    "* `max(*cols)`\n",
    "* `min(*cols)`\n",
    "* `sum(*cols)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userDF.groupBy(userDF[\"occupation\"]).avg(\"age\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `agg(*exprs)` to compute multiple aggregates and returns the result as a DataFrame. `exprs` can be a dict mapping from column name (string) to aggregate functions (string),\n",
    "\n",
    "The available aggregate functions are `avg`, `max`, `min`, `sum`, `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userDF.groupBy(\"gender\").agg({\"age\":\"avg\", \"*\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "2.1 Find all the movies that have average ratings greater than 4.0.\n",
    "\n",
    "2.2 Find all the movies that have average ratings greater than 4.0 and have recieved more than 400 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "# 2.1\n",
    "ratingDF.groupBy(\"movieId\")\\\n",
    ".avg(\"rating\").filter(\"avg(rating) > 4.0\")\\\n",
    ".show()\n",
    "\n",
    "# 2.2\n",
    "ratingDF.groupBy(\"movieId\")\\\n",
    ".agg({\"rating\":\"avg\", \"*\":\"count\"})\\\n",
    ".filter(\"avg(rating) > 4.0 AND count(1) > 400\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Converting Column Format\n",
    "\n",
    "Each column in a DataFrame has it own data format. Some time you need to convert column format. In the `ratingDF` DataFrame the timestamp column uses Unix time which is not human readable.\n",
    "\n",
    "`from_unixtime` converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "ratingDF = ratingDF.withColumn('timestamp', from_unixtime(ratingDF.timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingDF.printSchema()\n",
    "ratingDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further conver the string format into TimestampType by applying cast() to the column:\n",
    "\n",
    "`cast(dataType)`: Convert the column into type `dataType`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratingDF = ratingDF.withColumn('timestamp', ratingDF.timestamp.cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingDF.printSchema()\n",
    "ratingDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 User Defined Functions\n",
    "\n",
    "We can define our own function and apply it to a column in DataFrame. The following code creates a new column called \"ageGroup\" by applying the function `ageToGroup` to the `age` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define UDF\n",
    "def ageToGroup(num):\n",
    "    if num < 40:\n",
    "        return \"young\"\n",
    "    elif num > 64:\n",
    "        return \"old\"\n",
    "    else:\n",
    "        return \"middle\"\n",
    "    \n",
    "from pyspark.sql.functions import udf\n",
    "# Create a Column expression representing a UDF.\n",
    "udfAgeGroup=udf(ageToGroup, StringType())\n",
    "\n",
    "# apply UDF to age column\n",
    "userDF.withColumn(\"ageGroup\", udfAgeGroup(\"age\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Assume we want to evaluate the movies based on both the average rating and the review counts using the expression below:\n",
    "\n",
    "$\\exp{avg(rating)}\\times \\log{count(1)}$ \n",
    "\n",
    "Create a DataFrame that includes average rating, review count and score calculated based on the expression above for each movie. You may want to define a UDF and use your UDF to calculate the scores.\n",
    "\n",
    "What are the top 10 movies based on this score? You can use `DataFrame.sort(*cols, ascending)` to sort the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "import math\n",
    "\n",
    "def scoring(rating, cnt):\n",
    "    score = math.exp(rating) * math.log(cnt)\n",
    "    return score\n",
    "    \n",
    "# Create a Column expression representing a UDF.\n",
    "udfScoring=udf(scoring, FloatType())\n",
    "\n",
    "ratingAvg = ratingDF.groupBy(\"movieId\")\\\n",
    "    .agg({\"rating\":\"avg\", \"*\":\"count\"})\\\n",
    "    .withColumnRenamed(\"avg(rating)\", \"avgRating\")\\\n",
    "    .withColumnRenamed(\"count(1)\", \"reviewCnt\")\n",
    "\n",
    "# apply UDF to avgRating and reviewCnt\n",
    "ratingAvg\\\n",
    ".withColumn(\"score\", udfScoring(\"avgRating\", \"reviewCnt\"))\\\n",
    ".sort('score', ascending=False)\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 DataFrame NA Functions\n",
    "\n",
    "`Spark SQL` also provide simple functions for working with missing data in DataFrame.\n",
    "\n",
    "Let's first create a small DataFrame that contains some `na` fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = userDF.take(10)\n",
    "df.append(Row(age=None, gender=u'F', occupation=None, userId=11, zipCode=u'30329'))\n",
    "userDFna = spark.createDataFrame(df)\n",
    "userDFna.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`drop(how='any')` returns a new DataFrame omitting rows with null values. Parameter how can be ‘any’ or ‘all’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userDFna.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fill(value)` replaces null values. value can be a single value or a python dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A single value will fill the null values in the columns of the same type\n",
    "userDFna.na.fill(40).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userDFna.na.fill({'age': 30, 'occupation': u'other'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Using HiveQL in Spark SQL\n",
    "\n",
    "`SparkSession` in Spark 2.0 provides builtin support for Hive features including the ability to \n",
    "* write queries using HiveQL, \n",
    "* access to Hive UDFs, \n",
    "* and to read data from Hive tables.\n",
    "\n",
    "`SparkSession.sql(sqlQuery)` returns a DataFrame representing the result of the given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This query will return all the available tables, \n",
    "# including views as well as tables in Hive metastore.\n",
    "spark.sql(\"SHOW tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating Temporary Tables (Views)\n",
    "\n",
    "To use a DataFrame as a temporary Hive table (view), we can use `createOrReplaceTempView(name)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# register DataFrame userDF as user\n",
    "userDF.createOrReplaceTempView(\"userView\")\n",
    "\n",
    "spark.sql(\"SHOW tables\").show()\n",
    "spark.sql(\"SELECT * FROM userView WHERE occupation == 'programmer'\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Saving to Persistent Tables\n",
    "\n",
    "`DataFrames` can also be saved as persistent tables into Hive metastore using the saveAsTable command. If no Hive deployment exists then Spark will create a default local Hive metastore for you.\n",
    "\n",
    "`saveAsTable(name, mode=None)` saves the content of the DataFrame as the specified table. `mode` can be one of `append`, `overwrite`, `error`, `ignore` (default: error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# When executing savdAsTable:\n",
    "# a folder with the name `spark-warehouse` will be created in the same folder\n",
    "# under `spark-warehouse` there will be a subfolder with the name `user`.\n",
    "userDF.write.saveAsTable(\"user\", mode=\"overwrite\")\n",
    "spark.sql(\"SHOW tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "4.1 Register the ratingDF DataFrame you created in Exercise 1 as a View\n",
    "\n",
    "4.2 Find the top 20 users's info based on the total number of ratings they gave using SQL query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "# 4.1\n",
    "ratingDF.createOrReplaceTempView(\"ratingView\")\n",
    "spark.sql(\"SHOW tables\")\n",
    "\n",
    "# 4.2\n",
    "spark.sql(\"\"\"\n",
    "SELECT userView.*, userCount.cnt\n",
    "FROM userView JOIN \n",
    "    (SELECT userId, COUNT(*) AS cnt\n",
    "     FROM ratingView \n",
    "     GROUP BY userId) AS userCount\n",
    "ON userView.userId == userCount.userId\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 20\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Using User Defined Functions\n",
    "\n",
    "To use an UDF in SQL statements, we need to register the UDF with `registerFunction(name, f, returnType=StringType)`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.registerFunction(\"udfAgeGroup\", ageToGroup, StringType())\n",
    "spark.sql(\"SELECT *, udfAgeGroup(age) AS ageGroup FROM user\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 5\n",
    "\n",
    "Redo Exercise 3 with UDF in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "import math\n",
    "\n",
    "def scoring(rating, cnt):\n",
    "    score = math.exp(rating) * math.log(cnt)\n",
    "    return score\n",
    "    \n",
    "udfScoring=udf(scoring, FloatType())\n",
    "\n",
    "sqlContext.registerFunction(\"udfScoring\", scoring, FloatType())\n",
    "spark.sql(\"\"\"\n",
    "SELECT *, udfScoring(avgRating, reviewCnt) AS score FROM \n",
    "    (SELECT movieId, AVG(rating) AS avgRating, COUNT(1) AS reviewCnt\n",
    "     FROM ratingView\n",
    "     GROUP BY movieId)\n",
    "ORDER BY score DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
