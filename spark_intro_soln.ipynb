{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png\" align=left>\n",
    "<img src=\"http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png\" align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spark Tutorial: Learning Apache Spark**\n",
    "\n",
    "*Spark, like other big data tools, is powerful, capable, and well-suited to tackling a range of data challenges. Spark, like other big data technologies, is not necessarily the best choice for every data processing task.*\n",
    "\n",
    "## 0. Overview\n",
    " \n",
    "**0.1** Apache Spark is a fast and general engine for large-scale data processing:\n",
    "\n",
    "* Supports cyclic data flow and in-memory computing.\n",
    "* Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.\n",
    "* Offers over 80 high-level operators that make it easy to build parallel apps.\n",
    "\n",
    "![spark](http://spark.apache.org/images/logistic-regression.png)\n",
    "\n",
    "**0.2** Spark Libraries includes:\n",
    "\n",
    "* [SQL and DataFrames](http://spark.apache.org/sql/) \n",
    "* [ML and MLlib](http://spark.apache.org/mllib/) (machine learning)\n",
    "* [GraphX](http://spark.apache.org/graphx/) (graph) \n",
    "* [Spark Streaming](http://spark.apache.org/streaming/)\n",
    "* [SparkR (R on Spark)](http://spark.apache.org/docs/latest/sparkr.html)\n",
    "* and many third party libraries\n",
    "\n",
    "<img src=\"https://www.mapr.com/ebooks/spark/images/spark-stack-diagram.png\" width=\"60%\">\n",
    "\n",
    "You can combine these libraries seamlessly in the same application.\n",
    "\n",
    "**0.3** Spark Runs everywhere:\n",
    "\n",
    "* Standalone cluster mode\n",
    "* AWS EC2\n",
    "* Hadoop YARN\n",
    "* Apache Mesos\n",
    "* diverse data sources including HDFS, Cassandra, HBase, and S3.\n",
    "\n",
    "**0.4** This lecture will provide you a brief introduction on how to use Spark (with jupyter notebook). During this lecture we will cover:\n",
    "\n",
    "1. Initializing Spark\n",
    "2. RDDs, Transformations and Actions\n",
    "3. Working with Key-Value Pairs\n",
    "4. Performance & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell to setup data path\n",
    "import os\n",
    "\n",
    "datapath = os.getcwd()\n",
    "if datapath.find('databricks') != -1:\n",
    "    ACCESS_KEY = \"AKIAI2P5MSEO2JYXJVQQ\"\n",
    "    SECRET_KEY = \"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\"\n",
    "    AWS_BUCKET_NAME = \"nycdsabootcamp\"\n",
    "    datapath = \"s3a://%s:%s@%s/\" %(ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initializing Spark\n",
    "\n",
    "Every Spark application consists of a **driver program** that launches various parallel operations on executor Java Virtual Machines (JVMs) running either in a cluster or locally on the same machine. When running locally, `PySparkShell` is the driver program. \n",
    "\n",
    "Driver programs access Spark through a `SparkContext` object, which represents a connection to a computing cluster. A `SparkContext` object is the main entry point for Spark functionality.\n",
    "\n",
    "\n",
    "<img src=\"https://www.mapr.com/ebooks/spark/images/streaming-driver.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "In the PySpark shell, a special `SparkContext` is already created for you, in the variable called `sc`.\n",
    "\n",
    "Now run the following cell to make sure that Spark runs correctly with your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc type:  <class 'pyspark.context.SparkContext'>\n",
      "Driver Program name:  PySparkShell\n",
      "Spark version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Run the following command and check the output\n",
    "print \"sc type: \", type(sc)\n",
    "print \"Driver Program name: \", sc.appName\n",
    "print \"Spark version: \", sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RDDs, Transformations and Actions\n",
    "\n",
    "###  2.1 Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "The most fundamental Spark data structure is called `resilient distributed dataset` (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel.\n",
    "\n",
    "**Creating an RDD**\n",
    "\n",
    "Two basic ways to create RDDs: \n",
    "\n",
    "* `sc.parallelize(c) ` - parallelizing an existing collection in your driver program.\n",
    "* `sc.textFile(path)` - referencing a dataset in a storage source supported by Hadoop, including your local file system, HDFS, HBase, Amazon S3, etc.\n",
    "\n",
    "**count()** and **take(n)**\n",
    "\n",
    "Once an RDD has been created, we can use `RDD.count()` to check the number of elements and use `RDD.take(n)` to return the first n elements as a regular python list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of numRDD is:  100\n",
      "The first 5 elements are:  [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# xrange(100) will create a python iterator which generates numbers from 0 to 99\n",
    "\n",
    "numRDD = sc.parallelize(xrange(100))\n",
    "print \"The length of numRDD is: \", numRDD.count()\n",
    "print \"The first 5 elements are: \", numRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of textRDD is:  99\n",
      "The first 5 elements are:  [u'# Apache Spark', u'', u'Spark is a fast and general cluster computing system for Big Data. It provides', u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that', u'supports general computation graphs for data analysis. It also supports a']\n"
     ]
    }
   ],
   "source": [
    "# The README.md file contains the summary of spark\n",
    "\n",
    "filepath = os.path.join(datapath, \"pyspark_1/README.md\")\n",
    "\n",
    "textRDD = sc.textFile(filepath)\n",
    "print \"The length of textRDD is: \", textRDD.count()\n",
    "print \"The first 5 elements are: \", textRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collect()**\n",
    "\n",
    "To fetch the entire RDD to the driver node as a python list, we can use `RDD.collect()`. \n",
    "\n",
    "*Note*: this can cause the driver to run out of memory if the RDD is too big to fit into one single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "numList = numRDD.collect()\n",
    "print numList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data source is a folder containing many small files, and you want to keep the files being loaded separately then we can use another method called `wholeTextFiles`.\n",
    "\n",
    "`RDD.wholeTextFiles(path)` reads a directory of text files. Each file is read as a single record and returned in a key-value pair, where the key is the path of each file, the value is the content of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:\t13\n",
      "File path:\n",
      "\tfile:/Users/shuyan/Workspace/Supstat_NYC_office/bootcamp_slides/pyspark/pyspark_1/sent_items_lay_k/1.\n",
      "Content:\n",
      "Message-ID: <30661811.1075845189364.JavaMail.evans@thyme>\n",
      "Date: Wed, 30 May 2001 13:00:47 -0700 (PDT)\n",
      "From: kenneth.lay@enron.com\n",
      "To: tom.acton@enron.com, janie.aguayo@enron.com, amelia.alland@enron.com, \n",
      "\tlauri.allen@enron.com, aric.archie@enron.com, karl.atkins@enron.com, \n",
      "\ta..austin@enron.com, henry.batiste@enron.com, \n",
      "\trusty.belflower@enron.com, daxa.bhavsar@enron.com, \n",
      "\tmichael.bilberry@enron.com, brad.blevins@enron.com, \n",
      "\tdebbie.boudar@enron.com, greg.brazaitis@enron.com, \n",
      "\twillie.brooks@enron.com, rosa.brown@enron.com, \n",
      "\tjerry.bubert@enron.com, esther.buckley@enron.com, \n",
      "\tcandy.bywaters@enron.com, bob.camp@enron.com, howard.camp@enron.com, \n",
      "\tmolly.carriere@enron.com, clem.cernosek@enron.com, \n",
      "\tnick.cocavessis@enron.com, jane.coleman@enron.com, \n",
      "\tmary.comello@enron.com, robert.cook@enron.com, \n",
      "\twilliam.cosby@enron.com, paul.couvillon@enron.com, \n",
      "\trobert.crockett@enron.com, calvin.dodd@enron.com, \n",
      "\tjames.ducote@enron.com, cheryl.dudley@enron.com, \n",
      "\tlal.echterhoff@enron.com, michael.eiben@enron.com, \n",
      "\tanita.eisenbrandt@enron.com, melanie.facundo@enron.com, \n",
      "\tbob.fisher@enron.com, irene.flynn@enron.com, r.fosdick@enron.com, \n",
      "\tmelissa.graves@enron.com, ron.green@enron.com, \n",
      "\trebecca.griffin@enron.com, karen.gruesen@enron.com, \n",
      "\tjames.haden@enron.com, cynthia.hakemack@enron.com, \n",
      "\tjohn.handley@enron.com, gary.hanks@enron.com, reid.hansen@enron.com, \n",
      "\tcathy.harris@enron.com, maria.hernandez@enron.com, \n",
      "\tkatherine.herrera@enron.com, lisa.hesse@enron.com, \n",
      "\tnathan.hlavaty@enron.com, charles.howard@enron.com, \n",
      "\ttrisha.hughes@enron.com, monica.jackson@enron.com, \n",
      "\tkenneth.kaase@enron.com, kevin.kuehler@enron.com, \n",
      "\tgary.lamphier@enron.com, nancy.landry@enron.com, \n",
      "\tstacy.lesmeister@enron.com, e..lightfoot@enron.com, \n",
      "\tgerald.lofton@enron.com, a..lopez@enron.com, thi.ly@enron.com, \n",
      "\tsean.maki@enron.com, juanita.marchand@enron.com, \n",
      "\tcheryl.marshall@enron.com, eric.mason@enron.com, \n",
      "\tsaidia.matthews@enron.com, mark.mccoy@enron.com, \n",
      "\ttheresa.mckinney@enron.com, constance.mckinsey@enron.com, \n",
      "\tjulie.meyers@enron.com, yvette.miroballi@enron.com, \n",
      "\tjack.mitchell@enron.com, michael.morris@enron.com, \n",
      "\ttravis.moss@enron.com, joanie.ngo@enron.com, thu.nguyen@enron.com, \n",
      "\tmichael.olsen@enron.com, lee.papayoti@enron.com, \n",
      "\tmegan.parker@enron.com, jennifer.pattison@enron.com, \n",
      "\tdena.pawlowski@enron.com, eugene.peabody@enron.com, \n",
      "\tjack.peebles@enron.com, adriana.peterson@enron.com, \n",
      "\tjohn.peyton@enron.com, mary.poorman@enron.com, tess.ray@enron.com, \n",
      "\tbrian.redmond@enron.com, p..reinhardt@enron.com, \n",
      "\tcharlene.richmond@enron.com, chris.riegler@enron.com, \n",
      "\tbrian.riley@enron.com, leslie.robinson@enron.com, \n",
      "\tcarlos.rodriguez@enron.com, robert.rose@enron.com, \n",
      "\tbarbara.sargent@enron.com, vicente.sarmiento@enron.com, \n",
      "\thpl.schneider@enron.com, tom.shelton@enron.com, \n",
      "\tjack.simunek@enron.com, mary.smith@enron.com, susan.smith@enron.com, \n",
      "\tkenny.soignet@enron.com, karl.stewart@enron.com, l..taylor@enron.com, \n",
      "\tedward.terry@enron.com, s.thomas@enron.com, john.towles@enron.com, \n",
      "\tkeith.tyree@enron.com, kimberly.vaughn@enron.com, \n",
      "\telsa.villarreal@enron.com, joyce.viltz@enron.com, \n",
      "\tjudy.walters@enron.com, michael.walters@enron.com, \n",
      "\tkatie.washington@enron.com, george.weissman@enron.com, \n",
      "\tkam.welsch@enron.com, larry.white@enron.com, bud.wolcott@enron.com, \n",
      "\terica.wright@enron.com, sabrae.zajac@enron.com, \n",
      "\tjoe.zernicek@enron.com\n",
      "Subject: Thank You\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Lay, Kenneth </O=ENRON/OU=NA/CN=RECIPIENTS/CN=KLAY>\n",
      "X-To: Acton, Tom </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=87416512-97ec14e4-8625684a-57e620>, Aguayo, Janie </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=9ecbfebb-c962d1bd-862564bb-5483c3>, Alland, Amelia </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=c2ac4438-630996b1-86256509-56db66>, Allen, Lauri </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=e75399ca-e1ad213a-862564b4-4aa04f>, Archie, Aric </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=29c60cdb-656fad0f-86256a25-4d87bb>, Atkins, Karl </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=e862b4e5-e2152aec-8625687a-809bf1>, Austin, Jeffrey A. </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Jaustin>, Batiste, Henry </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=8ed2cca3-fbef6e82-862567d1-4da942>, Belflower, Rusty </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=55b447d8-b125fe79-86256922-4593eb>, Bhavsar, Daxa </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Dbhavsar>, Bilberry, Michael </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=1341353b-7d27d593-862564e3-6ebaaa>, Blevins, Brad </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=3e063ad6-713553c0-8625650b-5e2b34>, Boudar, Debbie </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=6b061463-e62e48f5-862566b3-51f6f9>, Brazaitis, Greg </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=fb9c714d-5351568d-8625653e-609cfb>, Brooks, Willie </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=493b91e1-935ad44c-862568dd-666622>, Brown, Rosa </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=32e2414e-f8d44774-862568cd-508b65>, Bubert, Jerry </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=a22c7f81-2b45255a-862569b9-76ab74>, Buckley, Esther </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=701bcae-23382d18-862564b5-652f14>, Bywaters, Candy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Cbywate>, Camp, Bob </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=346404f0-78702333-862567f4-734d8b>, Camp, Howard </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=4a52dea8-6a614cc9-862564b2-142569>, Carriere, Molly </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=9a3a40d1-1423d209-8625650b-5e316e>, Cernosek, Clem </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=c063b878-bdeab3ab-862564a4-65a766>, Cocavessis, Nick </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Ncocave>, Coleman, Jane </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Jcolema5>, Comello, Mary </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=7ec04d91-a1525b46-862568b0-566931>, Cook, Robert </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=57ffa86d-8c5db41d-86256516-799b52>, Cosby, William </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=40b25357-42f4584e-8625686d-58a676>, Couvillon, Paul </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=92f70f87-bf6d48c9-862568aa-7852e0>, Crockett, Robert </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=fff3e932-2b822d2a-8625650f-4f0529>, Dodd, Calvin </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=fe9c1a6a-daeaab31-862567f4-6eb9ff>, Ducote, James </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=14fd9900-eac6a711-862564ac-6794d7>, Dudley, Cheryl </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=4c23d312-f5273f83-862564b5-68e98a>, Echterhoff, Lal </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=bb2cd448-b78f98e9-86256510-55c564>, Eiben, Michael </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=8fe3512a-fc7dffaf-86256491-68df39>, Eisenbrandt, Anita </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Aeisenb>, Facundo, Melanie </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=cb527600-f4b8f1e7-862567ca-52acc1>, Fisher, Bob </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=47fb5e89-7fb976d1-862568e3-5b883d>, Flynn, Irene </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=38680348-aadaf41b-862564ed-673efb>, Fosdick, J R </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=3325a318-208db68d-862567f4-6eaa44>, Graves, Melissa </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Mgraves>, Green, Ron </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=ea5e111-ab56e187-8625687b-1ac9c>, Griffin, Rebecca </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=c0328c28-7db65cb3-862568fd-5f87d0>, Gruesen, Karen </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Kgruese>, Haden, James </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=d545daf1-7134ec07-8625675f-77b401>, Hakemack, Cynthia </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=f83df2bd-9b262624-86256500-6c4f0e>, Handley, John </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=426d0254-8a8286a4-86256510-563c5c>, Hanks, Gary </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=4de23ab0-f580b3db-862564b8-75471e>, Hansen, Reid </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=2ec45672-da96af25-862567f4-6e7b82>, Harris, Cathy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=e39584eb-7e043061-8625647c-6899a5>, Hernandez, Ana Maria </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Ahernand>, Herrera, Katherine </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=c8bcc5e7-d1f7d1e5-86256866-70b621>, Hesse, Lisa </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=32d41b4d-db4a82fc-862564b5-68defc>, Hlavaty, Nathan </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=7f25dda8-5fb246a3-862564b5-654cfa>, Howard, Charles </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=2ae91b5b-41bd5f60-8625698a-6f4c3c>, Hughes, Trisha </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=b58736ac-ec18e489-86256546-610b96>, Jackson, Monica </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=645116a0-e386bffd-8625687b-6a9ce7>, Kaase, Kenneth </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=53aa60f6-d713854c-86256513-6fc337>, Kuehler, Kevin </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=32d02f9a-81c21430-862567d7-5007e1>, Lamphier, Gary </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=c0624fb5-f41bf97b-862564b2-77a89f>, Landry, Nancy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Nlandry>, Lesmeister, Stacy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Slesmeis>, Lightfoot, Wayne E. </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Wlightf>, Lofton, Gerald </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=24c31842-a8a03055-862564c6-498e43>, Lopez, Blanca A. </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Blopez3>, Ly, Bac Thi </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=48091a90-c5e5b9c6-862567f4-6e6e76>, Maki, Sean </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Smaki>, Marchand, Juanita </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=7375038f-5cef2826-86256495-546e8f>, Marshall, Cheryl </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=e6169c82-23b36f9f-862564a7-60ae0a>, Mason, Eric </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=9a8e42b3-8112c7ea-88256885-52c2b8>, Matthews, Saidia </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=509d124d-6405320-86256966-589bf9>, McCoy, Mark </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=8cbe42fe-bb932212-862568cd-508882>, McKinney, Theresa </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=d43b0cba-9e38de5c-86256531-4d318c>, McKinsey, Constance </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Cmckinse>, Meyers, Julie </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=a8131d50-2229ac72-862564b4-7573b7>, Miroballi, Yvette </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Ymiroba>, Mitchell, Jack </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Jmitche>, Morris, Michael </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=125e0cf7-5802073d-862564b5-6561e7>, Moss, Travis </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=a3c3a3fc-619ba9fd-862568fd-5f8242>, Ngo, Joanie </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=6bb0429c-7b53b119-862565ce-15f8b>, Nguyen, Thu </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=f1a8a158-36aba6a3-862565ca-6e00b6>, Olsen, Michael </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Molsen2>, Papayoti, Lee </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=6adfefd3-aa5e79a0-8625654c-6c363c>, Parker, Megan </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=98fc2b4c-c89bfce4-86256854-712cc0>, Pattison, Jennifer </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=6d91920a-cbff4d71-86256498-7485d3>, Pawlowski, Dena </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Dpawlows>, Peabody, Eugene </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=b1c66e36-5ccc786d-862567f4-6e74a3>, Peebles, Jack </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Jpeebles>, Peterson, Adriana </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=bbbd9dd1-cb9e2693-86256491-4ced97>, Peyton, John </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=4f6f41d7-6f85666d-862566a6-58ef68>, Poorman, Mary </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=b95cfc52-7c3d17be-8625690a-741d5e>, Ray, Tess </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Tray2>, Redmond, Brian </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Bredmon>, Reinhardt, Donald P. </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Dreinha>, Richmond, Charlene </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=9eb4b4f6-a07b83a3-862564a5-6b5450>, Riegler, Chris </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=eb13d8c2-51ea4ca8-862568be-4daab0>, Riley, Brian </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=70b51a14-6bd54b96-86256515-4b9bef>, Robinson, Leslie </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=2a63e56a-ba43b42e-86256878-5083e4>, Rodriguez, Carlos </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=bc60f91e-65b70c9b-862564b3-60568e>, Rose, Robert </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=d7d51021-8295f4f1-862566b3-69ce4f>, Sargent, Barbara </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=a8da768d-bdb0f594-862564d5-6cd60c>, Sarmiento, Vicente </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=5b220f5a-34606589-862567f4-6e8fdb>, Schneider, Steve HPL </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=735e0ec4-de16cf36-86256515-4dc6b7>, Shelton, Tom </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=5ff9a281-e6dae784-8625653a-4ae08a>, Simunek, Jack </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=4dc317f7-70593a81-8625651b-58db24>, Smith, Mary </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=cf386750-cf591c05-86256491-68e916>, Smith, Susan </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Ssmith5>, Soignet, Kenny </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=8a2bb314-fe34074a-862564fb-73b709>, Stewart, Karl </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=a29546c3-c5a6aec7-862564c0-516e81>, Taylor, Vance L. </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Vtaylor>, Terry, Edward </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=7b023d70-6fbeacfb-86256498-4f8d6c>, Thomas, Yolanda S </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Ythomas3>, Towles, John </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=c5f9388f-4a36634b-862566b9-b0336>, Tyree, Keith </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=8d1af415-ff7f296d-862566b2-60a05c>, Vaughn, Kimberly </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=6e9f039d-26503aa0-862564b3-6bae74>, Villarreal, Elsa </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=31a6d895-997d0e5d-86256516-795e57>, Viltz, Joyce </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Jviltz>, Walters, Judy </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Jwalter>, Walters, Michael </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=c8c7bb33-d7b9336e-8625677e-5080dd>, Washington, Katie </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=cc0035c5-df6ae59e-86256998-5c461b>, Weissman, George </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=88372c14-8debaa6b-862564b5-68fd6f>, Welsch, Emma Kam </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=a6bf7c95-708d18b9-86256495-60d691>, White, Larry </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=98e7de04-15812026-86256870-5c392c>, Wolcott, Bud </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=1824004c-ffa8b8b2-8625686c-7a08e1>, Wright, Erica </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Ewright>, Zajac, Sabrae </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=c1480619-8a4d12b0-862566db-7accc0>, Zernicek, Joe </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Notesaddr/cn=a326b16e-91fb54fa-862567f4-6e95ef>\n",
      "X-cc: \n",
      "X-bcc: Wells, Tori L. </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Twells>, Fleming, Rosalee </O=ENRON/OU=NA/CN=RECIPIENTS/CN=Rflemin>\n",
      "X-Folder: \\Lay, Kenneth\\Lay, Kenneth\\Sent Items\n",
      "X-Origin: LAY-K\n",
      "X-FileName: Lay, Kenneth.pst\n",
      "\n",
      "I want to take this opportunity before the transfer of Houston Pipeline Company assets and employees to American Electric Power to thank each of you for your teamwork and success in our years together.\n",
      "\n",
      "Some of you were here when I joined your company in 1984, but many of you arrived during the Enron era and might not know the full story of my arrival.  I was president of Transco Energy at the time and was probably in line to be the chairman.  I did not expect to ever join another energy company.  Yet the directors of Houston Natural Gas felt like a new management team was needed for their company to prosper in a rapidly changing industry environment.  The quality of people I saw at HNG convinced me that the company would be a good platform from which to make major changes.  And the results over the last 17 years could not have been better.\n",
      "\n",
      "Houston Pipeline Company has been a major contributor to developing Enron into what it is today.  Your professionalism, ideas, dedication and commitment toward this company have been instrumental in making our success possible.  For this, you will not only be a critical part of Enron's history, but you will also always deserve Enron's gratitude.\n",
      "\n",
      "I wish you nothing but the very best in your next endeavors, even as I am sure you will not need my good luck wishes.  As always, your professionalism and dedication will be your best allies when facing new challenges and opportunities.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      " \n",
      "\n",
      "<Embedded Picture (Metafile)>\n"
     ]
    }
   ],
   "source": [
    "# `sent_items_Lay_k/` is a directory that contains emails sent by \n",
    "# Kenneth Lay, the CEO and chairman of Enron Corporation and one of the key \n",
    "# person in Enron scandal during his time in Enron Corporation.\n",
    "\n",
    "dirpath = os.path.join(datapath, \"./pyspark_1/sent_item_lay_k/\")\n",
    "\n",
    "fileRDD = sc.wholeTextFiles(dirpath)\n",
    "print \"Total number of files:\\t\", fileRDD.count()\n",
    "\n",
    "# The first element in fileRDD contains one entile file \n",
    "fileName, content = fileRDD.take(1)[0]\n",
    "print \"File path:\\n\\t\", fileName\n",
    "print \"Content:\\n\", content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "*1.1* Create an RDD of 1000 (or more) random numbers that satisfy uniform distribution on the interval [0, 1), call it `unifRDD`. Check your RDD by applying `.take(5)` to it. You may want to use python `random` module to create random numbers.\n",
    "\n",
    "*1.2* (Optional) Create a histogram with any plot functions you like to confirm that the numbers in `unifRDD` follows uniform distribution. *Note*: pyspark doesn't support plotting (yet) and you'll need to `collect()` the data to a python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1420716224881987, 0.9912027201017164, 0.018663027347752825, 0.19744660021743, 0.7923456800763063]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "from random import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1.1\n",
    "unifRDD = sc.parallelize([random() for i in xrange(1000)])\n",
    "print unifRDD.take(5)\n",
    "\n",
    "# 1.2\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.hist(unifRDD.collect(), 20, alpha = .4)\n",
    "plt.show()\n",
    "# uncomment the following line if you're using databricks\n",
    "# display(fig) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RDD Operations\n",
    "\n",
    "RDDs support two types of operations:\n",
    "\n",
    "* **transformation** - create a new dataset from an existing one. For example:\n",
    "    * `RDD.map(f)` - Return a new RDD by applying *f* to each element of this RDD.\n",
    "    * `RDD.filter(f)` - Return a new RDD containing only the elements that satisfy a predicate.\n",
    "* **action** - return a value to the driver program after running a computation on the dataset. For example:\n",
    "    * `RDD.take(n)` - Take the first n elements of the RDD.\n",
    "    * `RDD.count()` - Return the number of elements in this RDD.\n",
    "    * `RDD.collect()` - Return a list that contains all of the elements in this RDD. \n",
    "    * `RDD.reduce(f)` - Reduces the elements of this RDD using the specified commutative and associative binary operator.\n",
    "\n",
    "**.map(f)** \n",
    "\n",
    "`RDD.map(f)` returns a new RDD by applying a function to each element of this RDD.\n",
    "\n",
    "For example we can apply `math.sqrt()` function to each number of numRDD using `RDD.map()` transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 1.4142135623730951, 1.7320508075688772, 2.0, 2.23606797749979, 2.449489742783178, 2.6457513110645907, 2.8284271247461903, 3.0]\n"
     ]
    }
   ],
   "source": [
    "# Create a new RDD with the square root of each element in numRDD\n",
    "\n",
    "import math \n",
    "\n",
    "numRDDSqrt = numRDD.map(math.sqrt)\n",
    "print numRDDSqrt.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 0, 78, 75, 73, 74, 56, 42, 0, 26, 0, 0, 23, 0, 68, 76, 70, 56, 0, 17, 0, 62, 45, 0, 39, 0, 67, 0, 195, 66, 76, 151, 119, 0, 26, 0, 64, 0, 21, 0, 52, 0, 44, 0, 27, 0, 66, 0, 17, 0, 61, 0, 43, 0, 19, 0, 74, 74, 0, 29, 0, 32, 0, 75, 62, 41, 73, 72, 22, 0, 54, 0, 69, 0, 16, 0, 84, 17, 0, 19, 0, 33, 120, 0, 31, 0, 77, 76, 77, 0, 42, 120, 84, 65, 0, 16, 0, 97, 70]\n"
     ]
    }
   ],
   "source": [
    "# Create a new RDD that contains the string length of each element in textRDD\n",
    "\n",
    "textRDDlen = textRDD.map(len)\n",
    "print textRDDlen.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.reduce(f)**\n",
    "\n",
    "`RDD.reduce(f)` reduces the elements of this RDD using the specified *commutative and associative binary operator*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661.462947103\n"
     ]
    }
   ],
   "source": [
    "# Find the sum of the numbers in numRDDSqrt\n",
    "\n",
    "from operator import add\n",
    "\n",
    "sqrtSum = numRDDSqrt.reduce(add)\n",
    "print sqrtSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n"
     ]
    }
   ],
   "source": [
    "# Find the largest number in textRDDlen\n",
    "\n",
    "maxLine = textRDDlen.reduce(max)\n",
    "print maxLine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Passing Functions to Spark\n",
    "\n",
    "Spark’s API relies heavily on passing functions in the driver program to run on the cluster. We can define functions using python basic syntax:\n",
    "\n",
    "* Lambda expressions, for simple functions that can be written as an expression.\n",
    "* Functions defined using `def` for complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of words in each line \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 14, 13, 11, 12, 8, 6, 0, 1, 0, 0, 3, 0, 10, 6, 3, 8, 0, 3, 0, 6, 8, 0, 4, 0, 13, 0, 22, 10, 2, 8, 2, 0, 4, 0, 12, 0, 1, 0, 8, 0, 4, 0, 4, 0, 11, 0, 1, 0, 10, 0, 2, 0, 3, 0, 11, 11, 0, 2, 0, 6, 0, 12, 12, 9, 13, 14, 3, 0, 3, 0, 13, 0, 3, 0, 10, 4, 0, 1, 0, 7, 8, 0, 6, 0, 13, 11, 13, 0, 7, 4, 12, 8, 0, 2, 0, 6, 12]\n"
     ]
    }
   ],
   "source": [
    "def numOfWords(line):\n",
    "    words = line.split()\n",
    "    return len(words)\n",
    "\n",
    "textRDDwordlen = textRDD.map(numOfWords)\n",
    "print textRDDwordlen.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.filter(f)**\n",
    "\n",
    "`RDD.filter(f)` return a new RDD containing only the elements that satisfy a condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  2\n",
      "file:/Users/shuyan/Workspace/Supstat_NYC_office/bootcamp_slides/pyspark/pyspark_1/sent_items_lay_k/10.\n",
      "file:/Users/shuyan/Workspace/Supstat_NYC_office/bootcamp_slides/pyspark/pyspark_1/sent_items_lay_k/11.\n"
     ]
    }
   ],
   "source": [
    "# Jeff Skilling is the former CEO of the Enron Corporation, another key person in Enron scandal\n",
    "# Find all the emails that mentioned Jeff Skilling, \n",
    "# i.e., elememts in fileRDD that contain string \"Jeff Skilling\"\n",
    "\n",
    "fileRDDjs = fileRDD.filter(lambda kv: kv[1].find(\"Jeff Skilling\") > -1)\n",
    "\n",
    "print \"Total number of files: \", fileRDDjs.count()\n",
    "\n",
    "# The first element in fileRDD contains one entile file \n",
    "for sentfile in fileRDDjs.collect():\n",
    "    print sentfile[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Chaining Together Transformations and Actions\n",
    "\n",
    "Here're some key points in Spark that you need to remember:\n",
    "\n",
    "* An RDD is immutable, so once it is created, it cannot be changed. \n",
    "* Regular RDDs don't support random access (some third-party packages allow users to create indexedRDDs which support random access)\n",
    "* Each transformation creates a new RDD. \n",
    "* Spark uses lazy evaluation, so transformations are not actually executed until an action occurs.\n",
    "\n",
    "\n",
    "To perform multiple transformations we can chain them together using dot notation.\n",
    "\n",
    "Now let's calculate the sum of all the multiples of 3 or 5 below 1000 by chaining an RDD with a `filter` and a `reduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233168"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(xrange(1000)).filter(lambda x: x % 3 == 0 or x % 5 == 0).reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "*2.1* Calculate the sample variance of `unifRDD` uning proper transformations and actions. Compare your result with the built in function `RDD.variance()`. The formula is:\n",
    "\n",
    "$s^2 = \\frac{1}{N}\\sum\\limits_{i=1}^N(x_i - \\bar{x})^2$\n",
    "\n",
    "*2.2* Use Monte Carlo method to estimate Pi. Here are the suggested steps:\n",
    "\n",
    "* Create an RDD with each element a pair of uniformly distributed random numbers within [0, 1), call it `pointRDD`.\n",
    "* Create another RDD by filtering on `pointRDD` so that only those points that fall in the circle of radius 1 left, call it `pointInCircle`.\n",
    "* $\\pi$ can be estimated by `4 * pointInCircle.count() / pointRDD.count()`. Don't for get to convert `int` to `float`.\n",
    "\n",
    "You can chain multiple transformations and actions together if you feel comfortable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance (UDF) 0.0855947561078\n",
      "Variance (built-in function) 0.0855947561078\n",
      "Pi is approximately:  3.14193\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "\n",
    "from operator import add\n",
    "from random import random\n",
    "\n",
    "# 1.1\n",
    "m = sc.broadcast(unifRDD.mean())\n",
    "print \"Variance (UDF)\", unifRDD.map(lambda x: (x - m.value)**2).reduce(add) / (unifRDD.count())\n",
    "print \"Variance (built-in function)\", unifRDD.variance()\n",
    "\n",
    "# 1.2\n",
    "N = 10000000\n",
    "\n",
    "def inCircle(point):\n",
    "    x, y = point\n",
    "    return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "pi = 4.0 * sc.parallelize([(random(), random()) for x in range(N)]).map(inCircle).reduce(add) / N\n",
    "\n",
    "print \"Pi is approximately: \", pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 More Operations \n",
    "\n",
    "Let's investigate some common operations:\n",
    "* Actions:\n",
    "    * `takeOrdered(num, key=None)` - get the N elements from a RDD ordered in ascending order or as specified by the optional key function.\n",
    "    * `top(n, key=None)` - get the top N elements from a RDD.\n",
    "    * `takeSample(withReplacement, num, seed=None)` - return a fixed-size sampled subset of this RDD.\n",
    "    * `countByValue()` - return the count of each unique value in this RDD as a dictionary of (value, count) pairs.\n",
    "* Transformations:\n",
    "    * `flatMap(f)` - urn a new RDD by first applying a function to all elements of this RDD, and then flattening the results.\n",
    "    * `sample(withReplacement, fraction, seed=None)` - return a sampled subset of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "takeOrdered -  [0, 0, 2, 4, 5]\n",
      "top -  [10, 8, 6, 5, 5]\n",
      "takeSample -  [5, 4, 8, 0, 5]\n",
      "countByValue -  defaultdict(<type 'int'>, {0: 2, 2: 1, 4: 1, 5: 3, 6: 1, 8: 1, 10: 1})\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "rintRDD = sc.parallelize([randint(0, 10) for x in range(10)])\n",
    "\n",
    "print \"takeOrdered - \", rintRDD.takeOrdered(5)\n",
    "print \"top - \", rintRDD.top(5)\n",
    "print \"takeSample - \", rintRDD.takeSample(True, 5, 1)\n",
    "print \"countByValue - \", rintRDD.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatMap -  [u'#', u'Apache', u'Spark', u'Spark', u'is', u'a', u'fast', u'and', u'general', u'cluster']\n",
      "sample -  [u'Spark', u'guide,', u'using', u'of', u'Hadoop-supported', u'on']\n"
     ]
    }
   ],
   "source": [
    "wordRDD = textRDD.flatMap(lambda line: line.split())\n",
    "print \"flatMap - \", wordRDD.take(10)\n",
    "print \"sample - \", wordRDD.sample(False, 0.01, 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "A palindromic number reads the same both ways. The largest palindrome made from the product of two 2-digit numbers is 9009 = 91 × 99.\n",
    "\n",
    "Find the largest palindrome made from the product of two 3-digit numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906609"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "\n",
    "(sc.parallelize(xrange(100, 1000))\n",
    " .flatMap(lambda x: [x * y for y in range(x, 1000)])\n",
    " .filter(lambda x: str(x) == str(x)[::-1])\n",
    " .reduce(max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Working with Key-Value Pairs\n",
    "\n",
    "### 3.1 pair RDDs\n",
    "\n",
    "pair RDDs are a special type of RDDs in that each element consists of a Key/value pair.\n",
    "\n",
    "pair RDDs are commonly used to perform aggregations, and often we will do some initial ETL (extract, transform, and load) to get our data into a key/value format. \n",
    "\n",
    "To create a pair RDD, we can either apply `parallelize` on a list of tuples or use `map` to generate key-value pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('a', 2), ('b', 1)]\n",
      "[(0, 0), (1, 1), (0, 2), (1, 3), (0, 4)]\n"
     ]
    }
   ],
   "source": [
    "# parallelize on a list of tuples\n",
    "pairRDD1 = sc.parallelize([('a', 1), ('a', 2), ('b', 1)])\n",
    "print pairRDD1.collect()\n",
    "\n",
    "# generate key-value pairs using map\n",
    "pairRDD2 = numRDD.map(lambda x: (x % 2, x))\n",
    "print pairRDD2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Operations on pair RDDs\n",
    "\n",
    "Besides common operations, pair also RDDs expose many new operations. Two commonly used transformations on pair RDD are: `.groupByKey()` and `.reduceByKey()`.\n",
    "\n",
    "* `.groupByKey()` transformation gathers together pairs that have the same key and applies a function to two associated values at a time. \n",
    "* `.reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions. \n",
    "\n",
    "While both the `.groupByKey()` and `.reduceByKey()` transformations can often be used to solve the same problem and will produce the same answer, the `.reduceByKey()` transformation works much better for large distributed datasets.\n",
    "\n",
    "As an example, let's see how can we perform sum by key using the two transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98]), (1, [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99])]\n",
      "\n",
      "groupByKey -  [(0, 2450), (1, 2500)]\n",
      "reduceByKey - [(0, 2450), (1, 2500)]\n"
     ]
    }
   ],
   "source": [
    "# mapValues is used to convert iterable object to list for printing\n",
    "print pairRDD2.groupByKey().mapValues(lambda x: list(x)).collect()\n",
    " \n",
    "# mapValues sum values of the same list\n",
    "print \"\\ngroupByKey - \", pairRDD2.groupByKey().map(lambda (k, v): (k, sum(v))).collect()\n",
    "# pass add function into reduceByKey to perform summation\n",
    "print \"reduceByKey -\", pairRDD2.reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "*3.1* Create a pair RDD using `unifRDD` by mapping the numbers into 5 evenly spaced intervals.\n",
    "\n",
    "*3.2* Calculate the number of items of the same key. You may find the action [`countByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.countByKey) be useful.\n",
    "\n",
    "*3.3* Calculate the mean and sd of the elements that are in the same intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 205), (1, 212), (2, 183), (3, 208), (4, 192)]\n",
      "[(0, (0.10524500888264254, 0.05537537601805199)), (1, (0.3001178866330926, 0.057997358554036385)), (2, (0.5017401346072036, 0.05663129890821499)), (3, (0.7065108759867413, 0.0591974422875698)), (4, (0.904742886595292, 0.058376883471220664))]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "\n",
    "unifRDD = sc.parallelize([random() for i in xrange(1000)])\n",
    "\n",
    "# 3.1\n",
    "def getInteval(num):\n",
    "    return (int(num / .2), num)\n",
    "\n",
    "unifPairRDD = unifRDD.map(getInteval)\n",
    "\n",
    "# 3.2\n",
    "print unifPairRDD.countByKey().items()\n",
    "\n",
    "# 3.3\n",
    "def stat(v):\n",
    "    n = len(v)\n",
    "    mean = sum(v)/n\n",
    "    sd = (sum([(x - mean)**2 for x in v])/n)**.5\n",
    "    return (mean, sd)\n",
    "\n",
    "print unifPairRDD.groupByKey().map(lambda (k, v): (k, stat(v))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance & Optimization\n",
    "\n",
    "### 4.1 RDD Persistence\n",
    "\n",
    "Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small “hot” dataset or when running an iterative algorithm. You can mark an RDD to be persisted using the `persist()` or `cache()` methods on it.\n",
    "\n",
    "Please read [here](http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence) for more details about persistence and storage levels.\n",
    "\n",
    "###  4.2 Partitions\n",
    "\n",
    "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. \n",
    "\n",
    "When creating an RDD using `parallelize`, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize.\n",
    "\n",
    "The `textFile` method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 64MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value.\n",
    "\n",
    "Here are some actions that can give partition information in RDD:\n",
    "* `RDD.getNumPartitions()` - returns the number of partitions in RDD\n",
    "* `RDD.glom()` - transforms an RDD to a new RDD with elements within each partition coalesced into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numRDD num of partitions:  8\n",
      "textRDD num of partitions:  2\n"
     ]
    }
   ],
   "source": [
    "print \"numRDD num of partitions: \", numRDD.getNumPartitions()\n",
    "print \"textRDD num of partitions: \", textRDD.getNumPartitions() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Shared Variables \n",
    "\n",
    "**Broadcast Variables**\n",
    "\n",
    "Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.\n",
    "\n",
    "Broadcast variables have to be able to fit in memory on one machine. That means that they definitely should NOT be anything super large, like a large table or massive vector. Secondly, broadcast variables are immutable, meaning that they cannot be changed later on. This may seem inconvenient but it truly suits their use case. \n",
    "\n",
    "The following code shows how to use a broadcast variable to send values to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [1, 2, 3], [2, 4, 6], [3, 6, 9], [4, 8, 12]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar = sc.broadcast([1, 2, 3])\n",
    "bcRDD = sc.parallelize(range(5)).map(lambda x: [x * b for b in broadcastVar.value])\n",
    "bcRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accumulators**\n",
    "\n",
    "Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. To use accumulators, we need to use `.foreach(f)`, which applies a function to all elements of this RDD.\n",
    "\n",
    "The following example shows how to calculate $\\pi$ using an accumulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is approximately:  3.140732\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "N = 1000000\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def inCircle(point):\n",
    "    x, y = point\n",
    "    return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "sc.parallelize([(random(), random()) for x in range(N)]).foreach(lambda x: accum.add(inCircle(x)))\n",
    "print \"Pi is approximately: \", accum.value * 4.0 / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
